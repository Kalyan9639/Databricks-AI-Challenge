{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0da32a9e-3bb3-45ea-ad04-71455db87504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Streaming in Spark:\n",
    "\n",
    "Streaming is the capability that allows you to process real-time data at scale. Instead of processing a static \"batch\" of data (like a CSV file from last month), Spark Streaming processes data as it arrives from sources like Kafka, Flume, or IoT sensors.\n",
    "\n",
    "**Spark primarily uses a micro-batch architecture.**\n",
    "\n",
    "- It treats streaming as a series of very small, short-lived batch jobs.\n",
    "- The system collects data arriving within a specific time interval (e.g., every 1 second) and creates a small batch.\n",
    "- This batch is then processed by the Spark Engine using the same optimized logic used for batch processing.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "310cb958-32b3-4930-a886-f2b9fb16509a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Structured Streaming:\n",
    "\n",
    "Here, Instead of thinking about \"batches,\" you think about a continuously appended table.\n",
    "\n",
    "### How it works:\n",
    "- Input Table: Data arriving from the stream is viewed as new rows being appended to an unbounded input table.\n",
    "\n",
    "- Query: You define a query (Select, Filter, Aggregate) on this input table as if it were a static table.\n",
    "\n",
    "- Result Table: At every trigger interval, Spark checks for new data, processes it, and updates a \"Result Table.\"\n",
    "\n",
    "- Output: The updated result is pushed to a sink (like a database, console, or another Kafka topic).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9697cf66-a73d-4690-81b6-482b4eef558b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Typical Workflow:\n",
    "\n",
    "To build a Spark Streaming application, you follow these steps:\n",
    "\n",
    "1. Define Input Source: Connect to a source like Kafka, a TCP socket, or a folder of files.\n",
    "```python\n",
    "lines = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"host:port\").load()\n",
    "```\n",
    "\n",
    "2. Transform Data: Use standard Spark SQL or DataFrame operations.\n",
    "```python\n",
    "words = lines.selectExpr(\"CAST(value AS STRING)\").groupBy(\"value\").count()\n",
    "```\n",
    "\n",
    "3. Define Output Sink & Trigger: Decide where the data goes and how often to run the process.\n",
    "```python\n",
    "query = words.writeStream.outputMode(\"complete\").format(\"console\").start()\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d0e5a09-cfdb-47f8-906f-2a50dac4dbf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "In Databricks, the most common way to stream from a table in your catalog is using the `table()` method or specifying the delta format (since most Databricks tables are Delta tables).\n",
    "\n",
    "```python\n",
    "df = spark.readStream \\\n",
    "    .table(\"main.default.iot_data\")\n",
    "\n",
    "# Or using the delta format explicitly\n",
    "df = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .table(\"main.default.iot_data\")\n",
    "```\n",
    "\n",
    "| Format Category | Format Name | Use Case |  \n",
    "| :--- | :--- | :--- |  \n",
    "| **Databricks Standard** | `delta` | The most common for Databricks. Best for \"Bronze to Silver\" pipelines. |  \n",
    "| **Cloud Storage** | `cloudFiles` | **Auto Loader.** Highly recommended in Databricks for streaming files from S3/ADLS/GCS. |  \n",
    "| **File Sources** | `parquet`, `json`, `csv`, `orc`, `text` | Standard file formats. Note: these require a schema to be defined. |  \n",
    "| **Message Brokers** | `kafka` | Reading from Apache Kafka or Confluent. |  \n",
    "| **Azure Specific** | `eventhubs` | Direct connector for Azure Event Hubs. |  \n",
    "| **Testing/Dev** | `socket`, `rate` | `socket` is for text via port; `rate` generates dummy data for performance testing. |  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "270deb17-17b8-486d-a800-22fb585a9014",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Streaming in Spark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}