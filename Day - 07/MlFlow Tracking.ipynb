{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f8e9859-d719-4582-a35c-35b5d989d9ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### In this session, we are going to see how we are going to select the best model for production by comparing parameters and metrics. We will also see about the model versioning while changing the parameters. \n",
    "\n",
    "> ## Welcome To MLFlow Tracking\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b85b5d44-0498-4921-9e7a-60d6391a4de1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df = spark.read.load(\"/Volumes/dai/phase2/silver/train_df\")\n",
    "test_df = spark.read.load(\"/Volumes/dai/phase2/silver/test_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "675cc4be-34e0-426d-9d5d-f659be5944b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>CustomerID</th><th>total_spent</th><th>total_transactions</th><th>total_quantity</th><th>last_purchase_date</th><th>is_high_valued</th><th>class_weight</th><th>features</th></tr></thead><tbody><tr><td>null</td><td>1447682.1199996774</td><td>135080</td><td>269562</td><td>2011-12-09</td><td>1</td><td>2.418694690265487</td><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"1447682.1199996774\",\"135080.0\",\"269562.0\"]}</td></tr><tr><td>12346</td><td>0.0</td><td>2</td><td>0</td><td>2011-01-18</td><td>0</td><td>0.6302969155376189</td><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.0\",\"2.0\",\"0.0\"]}</td></tr><tr><td>12347</td><td>4309.999999999997</td><td>182</td><td>2458</td><td>2011-12-07</td><td>1</td><td>2.418694690265487</td><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"4309.999999999997\",\"182.0\",\"2458.0\"]}</td></tr><tr><td>12348</td><td>1797.24</td><td>31</td><td>2341</td><td>2011-09-25</td><td>0</td><td>0.6302969155376189</td><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"1797.24\",\"31.0\",\"2341.0\"]}</td></tr><tr><td>12349</td><td>1757.55</td><td>73</td><td>631</td><td>2011-11-21</td><td>0</td><td>0.6302969155376189</td><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"1757.55\",\"73.0\",\"631.0\"]}</td></tr><tr><td>12350</td><td>334.40000000000003</td><td>17</td><td>197</td><td>2011-02-02</td><td>0</td><td>0.6302969155376189</td><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"334.40000000000003\",\"17.0\",\"197.0\"]}</td></tr><tr><td>12352</td><td>1545.4100000000005</td><td>95</td><td>470</td><td>2011-11-03</td><td>0</td><td>0.6302969155376189</td><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"1545.4100000000005\",\"95.0\",\"470.0\"]}</td></tr><tr><td>12353</td><td>89.0</td><td>4</td><td>20</td><td>2011-05-19</td><td>0</td><td>0.6302969155376189</td><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"89.0\",\"4.0\",\"20.0\"]}</td></tr><tr><td>12355</td><td>459.4</td><td>13</td><td>240</td><td>2011-05-09</td><td>0</td><td>0.6302969155376189</td><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"459.4\",\"13.0\",\"240.0\"]}</td></tr><tr><td>12356</td><td>2811.4300000000007</td><td>59</td><td>1591</td><td>2011-11-17</td><td>1</td><td>2.418694690265487</td><td>{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"2811.4300000000007\",\"59.0\",\"1591.0\"]}</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         null,
         1447682.1199996774,
         135080,
         269562,
         "2011-12-09",
         1,
         2.418694690265487,
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"1447682.1199996774\",\"135080.0\",\"269562.0\"]}"
        ],
        [
         12346,
         0.0,
         2,
         0,
         "2011-01-18",
         0,
         0.6302969155376189,
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"0.0\",\"2.0\",\"0.0\"]}"
        ],
        [
         12347,
         4309.999999999997,
         182,
         2458,
         "2011-12-07",
         1,
         2.418694690265487,
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"4309.999999999997\",\"182.0\",\"2458.0\"]}"
        ],
        [
         12348,
         1797.24,
         31,
         2341,
         "2011-09-25",
         0,
         0.6302969155376189,
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"1797.24\",\"31.0\",\"2341.0\"]}"
        ],
        [
         12349,
         1757.55,
         73,
         631,
         "2011-11-21",
         0,
         0.6302969155376189,
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"1757.55\",\"73.0\",\"631.0\"]}"
        ],
        [
         12350,
         334.40000000000003,
         17,
         197,
         "2011-02-02",
         0,
         0.6302969155376189,
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"334.40000000000003\",\"17.0\",\"197.0\"]}"
        ],
        [
         12352,
         1545.4100000000005,
         95,
         470,
         "2011-11-03",
         0,
         0.6302969155376189,
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"1545.4100000000005\",\"95.0\",\"470.0\"]}"
        ],
        [
         12353,
         89.0,
         4,
         20,
         "2011-05-19",
         0,
         0.6302969155376189,
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"89.0\",\"4.0\",\"20.0\"]}"
        ],
        [
         12355,
         459.4,
         13,
         240,
         "2011-05-09",
         0,
         0.6302969155376189,
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"459.4\",\"13.0\",\"240.0\"]}"
        ],
        [
         12356,
         2811.4300000000007,
         59,
         1591,
         "2011-11-17",
         1,
         2.418694690265487,
         "{\"type\":\"1\",\"size\":null,\"indices\":null,\"values\":[\"2811.4300000000007\",\"59.0\",\"1591.0\"]}"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "CustomerID",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "total_spent",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "total_transactions",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "total_quantity",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "last_purchase_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "is_high_valued",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "class_weight",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "features",
         "type": "{\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"fields\":[{\"metadata\":{},\"name\":\"type\",\"nullable\":false,\"type\":\"byte\"},{\"metadata\":{},\"name\":\"size\",\"nullable\":true,\"type\":\"integer\"},{\"metadata\":{},\"name\":\"indices\",\"nullable\":true,\"type\":{\"containsNull\":false,\"elementType\":\"integer\",\"type\":\"array\"}},{\"metadata\":{},\"name\":\"values\",\"nullable\":true,\"type\":{\"containsNull\":false,\"elementType\":\"double\",\"type\":\"array\"}}],\"type\":\"struct\"},\"type\":\"udt\"}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9f35334-2638-46b5-9eaa-24bc924e0b2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> ##### We have to do the same as we did in previous day, i.e., Converting the features to be trained into one vector. I will use `RandomForest Classifier` here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f2785b3-3131-4391-b414-3856555d5a3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# assembler = VectorAssembler(\n",
    "#     inputCols=[\"total_spent\", \"total_transactions\", \"total_quantity\"],\n",
    "#     outputCol=\"features\"\n",
    "# )\n",
    "\n",
    "# train_df = assembler.transform(train_df)\n",
    "# test_df = assembler.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1077984-ca0c-4aff-b13d-497c0647c0a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"is_high_valued\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "529a7398-2365-4bd5-b479-47ee380ea09f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b709cd1-dc4b-4c6b-ae2b-358f57b85e07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"/Workspace/Users/kalyanmistcse@gmail.com/Databricks 14 Day AI Challenge - 02/Day - 07\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eba83445-eb54-443d-b95c-6cc83494b1ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/25 14:27:40 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.3.2) contains a local version label (+databricks.connect.17.3.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2026/02/25 14:27:43 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-b2de8d8d-c84c-4168-ab08-b1/tmpgmdeqpre/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\u001B[31m2026/02/25 14:27:43 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9999429070118346\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    num_trees = 46\n",
    "    max_depth = 7\n",
    "\n",
    "    rf = RandomForestClassifier(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"is_high_valued\",\n",
    "        weightCol=\"class_weight\",\n",
    "        numTrees=num_trees,\n",
    "        maxDepth=max_depth\n",
    "    )\n",
    "\n",
    "    model = rf.fit(train_df)\n",
    "    # Make predictions\n",
    "    predictions = model.transform(test_df)\n",
    "\n",
    "    # Evaluate\n",
    "    auc = evaluator.evaluate(predictions)\n",
    "\n",
    "    # log parameter\n",
    "    mlflow.log_params({\n",
    "        'numTrees': num_trees,\n",
    "        'maxDepth': max_depth\n",
    "    })\n",
    "\n",
    "    # log metrics\n",
    "    mlflow.log_metric(\"AUC-ROC\",auc)\n",
    "\n",
    "    # log model\n",
    "    mlflow.spark.log_model(model, \"random_forest_model\",dfs_tmpdir=\"/Volumes/dai/phase2/silver/tmp\")\n",
    "\n",
    "#     # log artifact\n",
    "#     mlflow.log_artifact(\"/dbfs/FileStore/tables/RF.png\")\n",
    "# # Load model\n",
    "# loaded_model = mlflow.spark.load_model(\"runs:/<run_id>/random_forest_model\")\n",
    "\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63452aba-638a-4449-8e74-60efd9c336b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/25 14:34:38 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.3.2) contains a local version label (+databricks.connect.17.3.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2026/02/25 14:34:40 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-b2de8d8d-c84c-4168-ab08-b1/tmpfauc4k7y/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\u001B[31m2026/02/25 14:34:40 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9999429070118346\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/25 14:34:52 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.3.2) contains a local version label (+databricks.connect.17.3.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2026/02/25 14:34:54 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-b2de8d8d-c84c-4168-ab08-b1/tmp3ocven45/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\u001B[31m2026/02/25 14:34:54 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9999592192941675\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/25 14:35:08 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.3.2) contains a local version label (+databricks.connect.17.3.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2026/02/25 14:35:10 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-b2de8d8d-c84c-4168-ab08-b1/tmpdsbvjo2b/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\u001B[31m2026/02/25 14:35:10 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9999102824471686\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/25 14:35:25 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.3.2) contains a local version label (+databricks.connect.17.3.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2026/02/25 14:35:27 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-b2de8d8d-c84c-4168-ab08-b1/tmpie6x7o4e/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\u001B[31m2026/02/25 14:35:27 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9999021263060022\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/25 14:35:41 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.3.2) contains a local version label (+databricks.connect.17.3.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2026/02/25 14:35:43 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-b2de8d8d-c84c-4168-ab08-b1/tmp5hbjifsh/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\u001B[31m2026/02/25 14:35:43 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9999347508706681\n"
     ]
    }
   ],
   "source": [
    "for i in [12,24,35,19,88]:\n",
    "    with mlflow.start_run():\n",
    "        num_trees = i\n",
    "        max_depth = 7\n",
    "\n",
    "        rf = RandomForestClassifier(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=\"is_high_valued\",\n",
    "            weightCol=\"class_weight\",\n",
    "            numTrees=num_trees,\n",
    "            maxDepth=max_depth\n",
    "        )\n",
    "\n",
    "        model = rf.fit(train_df)\n",
    "        # Make predictions\n",
    "        predictions = model.transform(test_df)\n",
    "\n",
    "        # Evaluate\n",
    "        auc = evaluator.evaluate(predictions)\n",
    "\n",
    "        # log parameter\n",
    "        mlflow.log_params({\n",
    "            'numTrees': num_trees,\n",
    "            'maxDepth': max_depth\n",
    "        })\n",
    "\n",
    "        # log metrics\n",
    "        mlflow.log_metric(\"AUC-ROC\",auc)\n",
    "\n",
    "        # log model\n",
    "        mlflow.spark.log_model(model, \"random_forest_model\",dfs_tmpdir=\"/Volumes/dai/phase2/silver/tmp\")\n",
    "\n",
    "    #     # log artifact\n",
    "    #     mlflow.log_artifact(\"/dbfs/FileStore/tables/RF.png\")\n",
    "    # # Load model\n",
    "    # loaded_model = mlflow.spark.load_model(\"runs:/<run_id>/random_forest_model\")\n",
    "\n",
    "    print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e049241-fc58-498b-b4b3-a410fc5f74fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### By comparing the models in the mlflow ui, i came to know that the model with `numTrees` in the range 20-30 gives best `AUC-ROC` score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6fc0788-5d74-4706-8758-a473f1a8a48f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/25 14:40:56 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.3.2) contains a local version label (+databricks.connect.17.3.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2026/02/25 14:40:58 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-b2de8d8d-c84c-4168-ab08-b1/tmp2wiwua_6/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\u001B[31m2026/02/25 14:40:58 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9998531894590031\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/25 14:41:12 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.3.2) contains a local version label (+databricks.connect.17.3.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2026/02/25 14:41:14 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-b2de8d8d-c84c-4168-ab08-b1/tmp6ft644zm/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\u001B[31m2026/02/25 14:41:14 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9999429070118345\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/25 14:41:27 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.3.2) contains a local version label (+databricks.connect.17.3.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2026/02/25 14:41:29 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-b2de8d8d-c84c-4168-ab08-b1/tmp_zbmkf84/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\u001B[31m2026/02/25 14:41:29 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9996900666356734\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/25 14:41:57 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.3.2) contains a local version label (+databricks.connect.17.3.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2026/02/25 14:42:00 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-b2de8d8d-c84c-4168-ab08-b1/tmpwxj49xyx/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\u001B[31m2026/02/25 14:42:00 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9997879403296713\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/25 14:42:11 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.3.2) contains a local version label (+databricks.connect.17.3.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2026/02/25 14:42:14 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-b2de8d8d-c84c-4168-ab08-b1/tmp99dnqlu8/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\u001B[31m2026/02/25 14:42:14 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9999592192941675\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/25 14:42:27 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.3.2) contains a local version label (+databricks.connect.17.3.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2026/02/25 14:42:30 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-b2de8d8d-c84c-4168-ab08-b1/tmppsp5qjn4/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\u001B[31m2026/02/25 14:42:30 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9998531894590031\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/25 14:42:42 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.3.2) contains a local version label (+databricks.connect.17.3.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2026/02/25 14:42:45 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-b2de8d8d-c84c-4168-ab08-b1/tmpglszuw8o/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\u001B[31m2026/02/25 14:42:45 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9999265947295015\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/25 14:42:58 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.3.2) contains a local version label (+databricks.connect.17.3.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2026/02/25 14:43:01 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-b2de8d8d-c84c-4168-ab08-b1/tmpeksof669/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\u001B[31m2026/02/25 14:43:01 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9998939701648356\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/25 14:43:14 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.3.2) contains a local version label (+databricks.connect.17.3.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2026/02/25 14:43:16 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-b2de8d8d-c84c-4168-ab08-b1/tmpd_oa1j0k/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\u001B[31m2026/02/25 14:43:16 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9998776578825026\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/25 14:43:31 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.3.2) contains a local version label (+databricks.connect.17.3.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2026/02/25 14:43:33 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-b2de8d8d-c84c-4168-ab08-b1/tmp9gcc_vxn/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\u001B[31m2026/02/25 14:43:33 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9999347508706681\n"
     ]
    }
   ],
   "source": [
    "for i in range(20,30):\n",
    "    with mlflow.start_run() as run:\n",
    "        num_trees = i\n",
    "        max_depth = 7\n",
    "\n",
    "        rf = RandomForestClassifier(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=\"is_high_valued\",\n",
    "            weightCol=\"class_weight\",\n",
    "            numTrees=num_trees,\n",
    "            maxDepth=max_depth\n",
    "        )\n",
    "\n",
    "        model = rf.fit(train_df)\n",
    "        # Make predictions\n",
    "        predictions = model.transform(test_df)\n",
    "\n",
    "        # Evaluate\n",
    "        auc = evaluator.evaluate(predictions)\n",
    "\n",
    "        # log parameter\n",
    "        mlflow.log_params({\n",
    "            'numTrees': num_trees,\n",
    "            'maxDepth': max_depth\n",
    "        })\n",
    "\n",
    "        # log metrics\n",
    "        mlflow.log_metric(\"AUC-ROC\",auc)\n",
    "\n",
    "        # log model\n",
    "        mlflow.spark.log_model(model, \"random_forest_model\",dfs_tmpdir=\"/Volumes/dai/phase2/silver/tmp\")\n",
    "\n",
    "        # Set unique tag for each run\n",
    "        mlflow.set_tag(\"unique_run_id\", f\"rf_{num_trees}_trees_{max_depth}_depth_{run.info.run_id}\")\n",
    "\n",
    "    print(\"AUC:\", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5844405a-2234-4f8c-b846-943479122a81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![image](/Workspace/Users/kalyanmistcse@gmail.com/Databricks 14 Day AI Challenge - 02/Day - 07/image.webp)\n",
    "---\n",
    "\n",
    "### As you can see in the above image, with `numTrees` = 24, I am getting the best performace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb923148-11ad-4409-88b5-9cb72c015df0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "MlFlow Tracking",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}